{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/revupnr/pytorch-cifar.git\n",
    "import sys\n",
    "sys.path.append(r'pytorch-cifar')\n",
    "sys.path.append(r'pytorch-cifar\\models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models import *\n",
    "from models.custom_resnet import Custom_ResNet18\n",
    "from utils import progress_bar\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from torch.utils.data import Dataset\n",
    "class AlbumentationImageDataset(Dataset):\n",
    "  def __init__(self, image_list, train= True):\n",
    "      self.image_list = image_list\n",
    "      self.aug = A.Compose({\n",
    "          A.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)),\n",
    "          A.HorizontalFlip(),\n",
    "          A.ShiftScaleRotate(),\n",
    "          A.CoarseDropout(1, 16, 16, 1, 16, 16,fill_value=0.473363, mask_fill_value=None),\n",
    "          A.ToGray()\n",
    "      })\n",
    "\n",
    "      self.norm = A.Compose({A.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)),\n",
    "      })\n",
    "      self.train = train\n",
    "        \n",
    "  def __len__(self):\n",
    "      return (len(self.image_list))\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "      \n",
    "      image, label = self.image_list[i]\n",
    "      \n",
    "      if self.train:\n",
    "        #apply augmentation only for training\n",
    "        image = self.aug(image=np.array(image))['image']\n",
    "      else:\n",
    "        image = self.norm(image=np.array(image))['image']\n",
    "      image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "      return torch.tensor(image, dtype=torch.float), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=False, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False, transform=transform_test)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "train_loader = torch.utils.data.DataLoader(AlbumentationImageDataset(trainset, train=True), batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(AlbumentationImageDataset(testset, train=False), batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False, num_workers=1)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_lr_finder\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "def find_lr(net, optimizer, criterion, train_loader):\n",
    "    \"\"\"Find learning rate for using One Cyclic LRFinder\n",
    "    Args:\n",
    "        net (instace): torch instace of defined model\n",
    "        optimizer (instance): optimizer to be used\n",
    "        criterion (instance): criterion to be used for calculating loss\n",
    "        train_loader (instance): torch dataloader instace for trainig set\n",
    "    \"\"\"\n",
    "    lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=5, num_iter=100, step_mode=\"exp\")\n",
    "    lr_finder.plot()\n",
    "    \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_finder = LRFinder(net, optimizer, criterion, device=device)\n",
    "lr_finder.range_test(train_loader, end_lr=10, num_iter=200)\n",
    "lr_finder.plot() \n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = min(lr_finder.history['loss'])\n",
    "ler_rate = lr_finder.history['lr'][np.argmin(lr_finder.history['loss'], axis=0)]\n",
    "print(\"Max LR is {}\".format(ler_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=ler_rate,\n",
    "                                                steps_per_epoch=len(train_loader), \n",
    "                                                epochs=24,\n",
    "                                                pct_start=0.2,\n",
    "                                                div_factor=10,\n",
    "                                                three_phase=False, \n",
    "                                                final_div_factor=50,\n",
    "                                                anneal_strategy='linear'\n",
    "                                                ) #final_div_factor=100,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, scheduler, optimizer, use_l1=False, lambda_l1=0.01):\n",
    "    \"\"\"Function to train the model\n",
    "\n",
    "    Args:\n",
    "        model (instance): torch model instance of defined model\n",
    "        device (str): \"cpu\" or \"cuda\" device to be used\n",
    "        train_loader (instance): Torch Dataloader instance for trainingset\n",
    "        criterion (instance): criterion to used for calculating the loss\n",
    "        scheduler (function): scheduler to be used\n",
    "        optimizer (function): optimizer to be used\n",
    "        use_l1 (bool, optional): L1 Regularization method set True to use . Defaults to False.\n",
    "        lambda_l1 (float, optional): Regularization parameter of L1. Defaults to 0.01.\n",
    "\n",
    "    Returns:\n",
    "        float: accuracy and loss values\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    lr_trend = []\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        # get samples\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Init\n",
    "        optimizer.zero_grad()\n",
    "        # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch \n",
    "        # accumulates the gradients on subsequent backward passes. Because of this, when you start your training loop, \n",
    "        # ideally you should zero out the gradients so that you do the parameter update correctly.\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model(data)\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_pred, target)\n",
    "\n",
    "        l1=0\n",
    "        if use_l1:\n",
    "            for p in model.parameters():\n",
    "                l1 = l1 + p.abs().sum()\n",
    "        loss = loss + lambda_l1*l1\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # updating LR\n",
    "        if scheduler:\n",
    "            if not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step()\n",
    "                lr_trend.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Update pbar-tqdm\n",
    "        pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        processed += len(data)\n",
    "        \n",
    "\n",
    "        pbar.set_description(desc= f'Batch_id={batch_idx} Loss={train_loss/(batch_idx + 1):.5f} Accuracy={100*correct/processed:0.2f}%')\n",
    "    return 100*correct/processed, train_loss/(batch_idx + 1), lr_trend\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    \"\"\"put model in eval mode and test it\n",
    "\n",
    "    Args:\n",
    "        model (instance): torch model instance of defined model\n",
    "        device (str): \"cpu\" or \"cuda\" device to be used\n",
    "        test_loader (instance): Torch Dataloader instance for testset\n",
    "        criterion (instance): criterion to used for calculating the loss\n",
    "\n",
    "    Returns:\n",
    "        float: accuracy and loss values\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    #iteration = len(test_loader.dataset)// test_loader.batch_size\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset), test_loss\n",
    "\n",
    "\n",
    "def save_model(model, epoch, optimizer, path):\n",
    "    \"\"\"Save torch model in .pt format\n",
    "\n",
    "    Args:\n",
    "        model (instace): torch instance of model to be saved\n",
    "        epoch (int): epoch num\n",
    "        optimizer (instance): torch optimizer\n",
    "        path (str): model saving path\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def fit_model(net, optimizer, criterion, device, NUM_EPOCHS,train_loader, test_loader, use_l1=False, scheduler=None, save_best=False):\n",
    "    \"\"\"Fit the model\n",
    "\n",
    "    Args:\n",
    "        net (instance): torch model instance of defined model\n",
    "        optimizer (function): optimizer to be used\n",
    "        criterion (instance): criterion to used for calculating the loss\n",
    "        device (str): \"cpu\" or \"cuda\" device to be used\n",
    "        NUM_EPOCHS (int): number of epochs for model to be trained\n",
    "        train_loader (instance): Torch Dataloader instance for trainingset\n",
    "        test_loader (instance): Torch Dataloader instance for testset\n",
    "        use_l1 (bool, optional): L1 Regularization method set True to use. Defaults to False.\n",
    "        scheduler (function, optional): scheduler to be used. Defaults to None.\n",
    "        save_best (bool, optional): If save best model to model.pt file, paramater validation loss will be monitered\n",
    "\n",
    "    Returns:\n",
    "        (model, list): trained model and training logs\n",
    "    \"\"\"\n",
    "    training_acc, training_loss, testing_acc, testing_loss = list(), list(), list(), list()\n",
    "    lr_trend = []\n",
    "    if save_best:\n",
    "        min_val_loss = np.inf\n",
    "        save_path = 'model.pt'\n",
    "\n",
    "    for epoch in range(1,NUM_EPOCHS+1):\n",
    "        print(\"EPOCH: {} (LR: {})\".format(epoch, optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        train_acc, train_loss, lr_hist = train(\n",
    "            model=net, \n",
    "            device=device, \n",
    "            train_loader=train_loader, \n",
    "            criterion=criterion ,\n",
    "            optimizer=optimizer, \n",
    "            use_l1=use_l1, \n",
    "            scheduler=scheduler\n",
    "        )\n",
    "        test_acc, test_loss = test(net, device, test_loader, criterion)\n",
    "        # update LR\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(test_loss)\n",
    "        \n",
    "        if save_best:\n",
    "            if test_loss < min_val_loss:\n",
    "                print(f'Valid loss reduced from {min_val_loss:.5f} to {test_loss:.6f}. checkpoint created at...{save_path}\\n')\n",
    "                save_model(net, epoch, optimizer, save_path)\n",
    "                min_val_loss = test_loss\n",
    "            else:\n",
    "                print(f'Valid loss did not inprove from {min_val_loss:.5f}\\n')\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "        training_acc.append(train_acc)\n",
    "        training_loss.append(train_loss)\n",
    "        testing_acc.append(test_acc)\n",
    "        testing_loss.append(test_loss)\n",
    "        lr_trend.extend(lr_hist)    \n",
    "\n",
    "    if scheduler:   \n",
    "        return net, (training_acc, training_loss, testing_acc, testing_loss, lr_trend)\n",
    "    else:\n",
    "        return net, (training_acc, training_loss, testing_acc, testing_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, history = fit_model(\n",
    "    net=net, device=device,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    train_loader=train_loader, test_loader=test_loader,\n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler, \n",
    "    NUM_EPOCHS=24\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60192f4408c2d32409087940ab98beb811e9d4c35c7d51cd7e2b71b4963347f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
